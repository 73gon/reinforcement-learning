{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Prof. Milica Gašić"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo and Temporal Difference Prediction\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import rl_tests\n",
    "from random import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented the environment for generating random walk episodes below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk_rollout(n_states=5, left_reward=0, right_reward=1):\n",
    "    terminal_states = [0, n_states-1]\n",
    "    s = n_states//2 # start in the middle\n",
    "    e = [s]   # episode\n",
    "    while s not in terminal_states:\n",
    "        # do random action 'right' or 'left'\n",
    "        s += 1 if random() > 0.5 else -1     # next state\n",
    "        if s == n_states - 1:\n",
    "            r = right_reward\n",
    "        elif s == 0:\n",
    "            r = left_reward\n",
    "        else:\n",
    "            r = 0\n",
    "        e.extend([float(r), s])\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to implement value iteration for computing value estimates for the episodes. Start with implementing the update per episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_step_inc(V, e, gamma, alpha):\n",
    "    #######################################################################\n",
    "    # TODO  implement monte carlo prediction update for one episode       #\n",
    "    # update the variable V                                               #\n",
    "    #######################################################################\n",
    "    \n",
    "    #######################################################################\n",
    "    # End of your code.                                                   #\n",
    "    #######################################################################\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def TD_step(V, e, gamma, alpha):\n",
    "    #######################################################################\n",
    "    # TODO  implement TD prediction update for one episode                #\n",
    "    # update the variable V                                               #\n",
    "    #######################################################################\n",
    "    \n",
    "    #######################################################################\n",
    "    # End of your code.                                                   #\n",
    "    #######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_random_walk_updates():\n",
    "    # Setup a fixed context for reproducible testing\n",
    "    # 5 states total: 0 (Left Term), 1, 2, 3, 4 (Right Term)\n",
    "    n_states = 5\n",
    "    gamma = 0.9\n",
    "    alpha = 0.1\n",
    "    # A fixed episode: Start at 2, go Left(1), Right(2), Right(3), Right(4-Term)\n",
    "    # Format: [s, r, s, r, s, r, s, r, s]\n",
    "    # Note: The rewards are 0 unless hitting terminal state 4 (r=1)\n",
    "    # fixed_episode = [[2, 0.0, 1, 0.0, 2, 0.0, 3, 1.0, 4], [2, 0.0, 1, 0.0, 0]]\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    yield 'MC_step_inc()'\n",
    "    # ---------------------------------------------------------\n",
    "    # Expected calculation for MC (Every-visit):\n",
    "    # t=6 (s=3, r=1): G=1.0. V[3] += 0.1*(1.0 - 0.5) -> 0.55\n",
    "    # t=4 (s=2, r=0): G=0.9. V[2] += 0.1*(0.9 - 0.5) -> 0.54\n",
    "    # t=2 (s=1, r=0): G=0.81. V[1] += 0.1*(0.81 - 0.5) -> 0.531\n",
    "    # t=0 (s=2, r=0): G=0.729. V[2] += 0.1*(0.729 - 0.54) -> 0.5589\n",
    "    # Sum = 0.531 + 0.5589 + 0.55 = 1.6399\n",
    "    for fixed_episode, expected_mc_sum in zip([[2, 0.0, 1, 0.0, 2, 0.0, 3, 1.0, 4], [2, 0.0, 1, 0.0, 0]], [1.6399, 1.4000]):\n",
    "        # Initialize V with 0.5 for non-terminals\n",
    "        V_mc = np.array([0.0, 0.5, 0.5, 0.5, 0.0])\n",
    "\n",
    "        MC_step_inc(V_mc, fixed_episode, gamma, alpha)\n",
    "\n",
    "        # Verify shape/type using your framework's helper\n",
    "        if (yield from rl_tests.check_numpy_array(V_mc, name='V', shape=(n_states,), dtype=np.floating)):\n",
    "            v_sum = np.sum(V_mc)\n",
    "            yield np.isclose(v_sum, expected_mc_sum, atol=1e-5), \\\n",
    "                f'The MC value updates are incorrect. Expected sum ~{expected_mc_sum:.4f}, got {v_sum:.4f}'\n",
    "        yield None\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    yield 'TD_step()'\n",
    "    # ---------------------------------------------------------\n",
    "    # Expected calculation for TD:\n",
    "    # t=0 (s=2->1): Tgt=0+0.9*0.5=0.45. V[2] += 0.1*(0.45-0.5) -> 0.495\n",
    "    # t=2 (s=1->2): Tgt=0+0.9*0.495=0.4455. V[1] += 0.1*(0.4455-0.5) -> 0.494555\n",
    "    # t=4 (s=2->3): Tgt=0+0.9*0.5=0.45. V[2] += 0.1*(0.45-0.495) -> 0.4905\n",
    "    # t=6 (s=3->4): Tgt=1+0.9*0=1.0. V[3] += 0.1*(1.0-0.5) -> 0.55\n",
    "    # Sum = 0.494555 + 0.4905 + 0.55 = 1.535055\n",
    "    for fixed_episode, expected_td_sum in zip([[2, 0.0, 1, 0.0, 2, 0.0, 3, 1.0, 4], [2, 0.0, 1, 0.0, 0]], [1.535055, 1.4450]):\n",
    "\n",
    "        # Reset V for TD test\n",
    "        V_td = np.array([0.0, 0.5, 0.5, 0.5, 0.0])\n",
    "\n",
    "        TD_step(V_td, fixed_episode, gamma, alpha)\n",
    "\n",
    "        if (yield from rl_tests.check_numpy_array(V_td, name='V', shape=(n_states,), dtype=np.floating)):\n",
    "            v_sum = np.sum(V_td)\n",
    "            yield np.isclose(v_sum, expected_td_sum, atol=1e-5), \\\n",
    "                f'The TD value updates are incorrect. Expected sum ~{expected_td_sum:.4f}, got {v_sum:.4f}'\n",
    "        yield None\n",
    "\n",
    "# Run the tests\n",
    "rl_tests.run_tests(test_random_walk_updates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to track how our value estimates converge over time. \n",
    "We will rollout a number of episodes, and update our estimates using the function implemented above.\n",
    "We iterate this episode sampling and evaluation maxiter amount of times\n",
    "We would like to plot the convergence of each method, so we keep the error to be plotted later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def RMSError(v0, v1):\n",
    "    return np.sqrt(((v0-v1)**2).mean())\n",
    "\n",
    "def mc_compute_v(n_episodes, n_states, alpha, maxiter):\n",
    "    \n",
    "    rms = np.zeros(n_episodes) #list to collect rms for each episode\n",
    "\n",
    "    # iterate maxiter amount of times\n",
    "    for k in range(maxiter):\n",
    "        # roll out episodes\n",
    "        episodes = []\n",
    "        for _ in range(n_episodes):\n",
    "            episodes.append(random_walk_rollout(n_states = n_states))\n",
    "\n",
    "        #initialize containers\n",
    "        v = np.zeros(n_states)\n",
    "        \n",
    "        #######################################################################\n",
    "        # TODO loop through the episodes and                                  #\n",
    "        # call the episode-wise MC update you have implemented above          #\n",
    "        # for each iteraction, accummulate the rms per episode                #\n",
    "        #######################################################################\n",
    "\n",
    "        \n",
    "            \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "    \n",
    "    rms /= maxiter # average the rms\n",
    "    \n",
    "    return v, rms\n",
    "    \n",
    "\n",
    "def td_compute_v(n_episodes, n_states, alpha, maxiter):\n",
    "    \n",
    "    rms = np.zeros(n_episodes) #list to collect rms for each episode\n",
    "\n",
    "    # iterate maxiter amount of times\n",
    "    for k in range(maxiter):\n",
    "        # roll out episodes\n",
    "        episodes = []\n",
    "        for _ in range(n_episodes):\n",
    "            episodes.append(random_walk_rollout(n_states = n_states))\n",
    "            \n",
    "        v = np.zeros(n_states)\n",
    "        \n",
    "        #######################################################################\n",
    "        # TODO call the episode-wise TD update you have implemented above     #\n",
    "        # and maintain a list tracking the RMS error of the estimate          #\n",
    "        # for each iteraction, accummulate the rms per episode                #\n",
    "        #######################################################################\n",
    "\n",
    "        \n",
    "\n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "    \n",
    "    rms /= maxiter # average the rms\n",
    "    \n",
    "    return v, rms\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the functions above to answer the questions:\n",
    "* How does MC and TD differs?\n",
    "* What is the influence of alpha on the convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = []\n",
    "\n",
    "gamma = 1.0\n",
    "n_states = 5\n",
    "n_episodes = 100\n",
    "maxiter = 100\n",
    "\n",
    "td_alphas=[0.15, 0.2, 0.25]\n",
    "mc_alphas=[0.01, 0.02, 0.03, 0.04]\n",
    "\n",
    "v_true = np.linspace(0, 1, num=n_states)\n",
    "v_true[-1] = 0\n",
    "\n",
    "# rollout episodes\n",
    "#for _ in range(n_episodes):\n",
    "#    episodes.append(random_walk_rollout(n_states = n_states))\n",
    "\n",
    "# value estimates with MC:\n",
    "print(\"\\n====== MC ======\\n\")\n",
    "for alpha in mc_alphas:\n",
    "    print(f\"alpha:{alpha}\")\n",
    "    v, rms = mc_compute_v(n_episodes, n_states, alpha, maxiter)\n",
    "    plt.plot(rms, '--', label=f'MC {alpha}')\n",
    "    print(v)\n",
    "\n",
    "# value estimates with TD:\n",
    "print(\"\\n====== TD ======\\n\")\n",
    "for alpha in td_alphas:\n",
    "    print(f\"alpha:{alpha}\")\n",
    "    v, rms = td_compute_v(n_episodes, n_states, alpha, maxiter)\n",
    "    plt.plot(rms, label=f'TD {alpha}')\n",
    "    print(v)\n",
    "\n",
    "plt.xlabel('walks')\n",
    "plt.ylabel('RMS error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_exc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
